{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import GetOldTweets3 as got \n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "from ipywidgets import widgets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "global KEYWORDS\n",
    "global DF\n",
    "global ANALYSIS\n",
    "global TEXT_SOURCE\n",
    "global WORDS\n",
    "global TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_url(url):\n",
    "    if url:\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "            return \" \".join([p.get_text().replace(u'\\xa0', u' ') for p in soup.find_all('p')])\n",
    "        except:\n",
    "            return \"\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topics[topic_idx] = \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return topics\n",
    "\n",
    "def get_topics(texts, \n",
    "               n_components=20, \n",
    "               n_top_words = 20, \n",
    "               n_features=int(1e12),\n",
    "               keywords=[],\n",
    "               topic_model='NMF'\n",
    "              ):\n",
    "    vect = CountVectorizer(max_df=0.8, \n",
    "                           min_df=1,\n",
    "                           max_features=n_features, \n",
    "                           stop_words=get_stop_words('de') + keywords)\n",
    "    if topic_model == 'LDA':\n",
    "        model = LatentDirichletAllocation(n_components=n_components, max_iter=200,\n",
    "                                learning_method='online',\n",
    "                                random_state=0)\n",
    "    elif topic_model == 'NMF':\n",
    "        model = NMF(n_components=n_components, random_state=0)\n",
    "    tf = vect.fit_transform(texts)\n",
    "    model.fit(tf)\n",
    "    topic_assignments = model.transform(tf)\n",
    "    tf_feature_names = vect.get_feature_names()\n",
    "    topics_dict = get_top_words(model, tf_feature_names, n_top_words)\n",
    "    \n",
    "    # sort topics by occurrence\n",
    "    topic_loadings = topic_assignments.sum(axis=0).argsort()[:-1:]\n",
    "    topic_assignments = topic_assignments[:,topic_loadings]\n",
    "    topics = [topics_dict[topic_idx] for topic_idx in topic_loadings]\n",
    "    return topic_assignments.argmax(axis=1), topics\n",
    "\n",
    "def get_top_word_counts(texts, n_top_words, keywords):\n",
    "    vect = CountVectorizer(max_df=1., \n",
    "                           min_df=1,\n",
    "                           max_features=n_top_words, \n",
    "                           stop_words=get_stop_words('de') + keywords)\n",
    "    wordcounts = vect.fit_transform(texts)\n",
    "    return wordcounts, vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Analyse mit Topic Models (oder ohne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Runterladen\n",
    "\n",
    "Lade Tweets aus deutschsprachigem Raum für den angegebenen Zeitraum, die die angegebenen keywords enthalten.\n",
    "\n",
    "Wenn maxTweets auf 0 gesetzt ist, wird versucht alle zu holen. Das kann dauern, und manchmal ist man danach gedrosselt. \n",
    "\n",
    "Klicke ``Run Interact`` button zum ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884af99fe253488ea1684356a7a56e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='covid klopapier', description='keywords'), DatePicker(value=Timestamp('2020-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact_manual(maxTweets=(0, 100))\n",
    "def get_tweets(keywords='covid klopapier', \n",
    "               start=widgets.DatePicker(value=pd.to_datetime('2020-04-10')), \n",
    "               stop=widgets.DatePicker(value=pd.to_datetime('2020-04-16')), \n",
    "               maxTweets=10):\n",
    "    print(f'Fetching tweets for keywords: {keywords}')\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(keywords)\\\n",
    "                                             .setSince(start.strftime('%Y-%m-%d'))\\\n",
    "                                             .setUntil(stop.strftime('%Y-%m-%d'))\\\n",
    "                                             .setMaxTweets(maxTweets)\\\n",
    "                                             .setLang('de')\\\n",
    "                                             .setNear('Berlin, Germany')\\\n",
    "                                             .setWithin('1000km') \n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "    tweet_dicts = []\n",
    "    for tweet in tweets:\n",
    "        tweet_dict = tweet.__dict__ \n",
    "        tweet_dict['url_text'] = get_text_from_url(tweet.urls)\n",
    "        tweet_dicts.append(tweet_dict)\n",
    "        \n",
    "    print(f'Found {len(tweets)} tweets for keywords: {keywords}')\n",
    "    global DF\n",
    "    DF = pd.DataFrame(tweet_dicts).set_index('date')\n",
    "    global KEYWORDS\n",
    "    KEYWORDS = re.split(\"[\" + string.punctuation + \" \\t\\n]+\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Analysieren\n",
    "\n",
    "- ``text_source``: Wähle, ob Text aus tweets oder den in tweets verlinkten Artikeln analysiert werden soll. \n",
    "- ``analysis``: Wähle ob Häufigkeit einzelner Worte oder Topics analysiert werden.\n",
    "- ``n_top_words``: wie viele Worte berücksichtigt werden oder bei Topic Analysis, wieviele Wòrter angezeigt werden\n",
    "- ``topic_model``: Art des Topic Models (Latent Dirichlet Allocation oder Nonnegative Matrix Factorization) - wird ignoriert, wnen ``analysis`` auf ``wordcount`` gesetzt ist\n",
    "- ``num_topics``: Wie viele Topics extrahiert werden soll.\n",
    "\n",
    "Klicke ``Run Interact`` button zum ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577efe537fff4249b1c892018a7185f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='text_source', options=('tweets', 'urls in tweets'), value='tweets'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact_manual(num_topics=(2, 20),\n",
    "                         text_source=['tweets','urls in tweets'],\n",
    "                         analysis=['topic model', 'wordcounts'],\n",
    "                         n_top_words = (5, 20),\n",
    "                         topic_model=['LDA','NMF']\n",
    "                        )\n",
    "def plot_tweets(\n",
    "         text_source='tweets',\n",
    "         analysis='topic model',\n",
    "         n_top_words=5,\n",
    "         topic_model='NMF',\n",
    "         num_topics=10    \n",
    "        ):\n",
    "    global DF\n",
    "    global TOPICS\n",
    "    global ANALYSIS \n",
    "    ANALYSIS = analysis\n",
    "    global TEXT_SOURCE\n",
    "    TEXT_SOURCE = text_source\n",
    "    if len(DF) == 0:\n",
    "        print(\"Get some tweets first.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Analysing {len(DF)} {text_source} with {analysis}, exluding keywords: {KEYWORDS}\")\n",
    "        \n",
    "        if text_source == 'tweets':\n",
    "            text_col = 'text'\n",
    "        elif text_source == 'urls in tweets':            \n",
    "            text_col = 'url_text'\n",
    "            \n",
    "        if analysis == 'topic model':\n",
    "            DF['topic'], TOPICS = get_topics(DF[text_col], \n",
    "                                                    n_components=num_topics, \n",
    "                                                    n_top_words=n_top_words,\n",
    "                                                    keywords=KEYWORDS,\n",
    "                                                    topic_model=topic_model\n",
    "                                                  )\n",
    "            pd.get_dummies(DF['topic']).resample('D').sum().plot(marker='o')\n",
    "            plt.legend([f'Topic {idx}: {text[:10]} ...' for idx, text in enumerate(TOPICS)]);\n",
    "            print(\"\\n\".join([f'Topic {idx}: {text}' for idx, text in enumerate(TOPICS)]))\n",
    "        elif analysis == 'wordcounts':\n",
    "            word_counts, words = get_top_word_counts(DF[text_col],  \n",
    "                                                     n_top_words=n_top_words,\n",
    "                                                    keywords=KEYWORDS)\n",
    "            global TOP_WORDS\n",
    "            TOP_WORDS = words\n",
    "            pd.DataFrame(word_counts.toarray(), \n",
    "                         index=DF.index, \n",
    "                         columns=words).resample('D').sum().plot(marker='o')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Anschauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578457b070794583943c2539da742e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Select(description='contains', options=('All', 'corona', 'coronavirus', 'coronavirusde',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.html.widgets import interactive\n",
    "\n",
    "df = DF.copy(deep=True)\n",
    "if TEXT_SOURCE == 'tweets':\n",
    "    text_col = 'text'\n",
    "elif TEXT_SOURCE == 'urls in tweets':\n",
    "    text_col = 'url_text'\n",
    "\n",
    "if ANALYSIS == 'wordcounts':\n",
    "    items = ['All'] + sorted(TOP_WORDS)\n",
    "elif ANALYSIS == 'topic model':\n",
    "    items = ['All'] + [f'{idx}: {text[:20]} ...' for idx, text in enumerate(TOPICS)]\n",
    "    \n",
    "def make_clickable(val):\n",
    "    # target _blank to open new window\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val, val)\n",
    "    \n",
    "def view(contains=''):\n",
    "    if contains=='All': \n",
    "        print(f'Showing all {len(df)} tweets')\n",
    "        display(df.loc[:,['username','to','text','permalink']].style.format({'permalink': make_clickable}))\n",
    "    elif ANALYSIS == 'wordcounts':\n",
    "        idx = df[text_col].str.lower().str.contains(contains.lower())\n",
    "        print(f'Found {idx.sum()} tweets')\n",
    "        display(df.loc[idx,['username','to','text','permalink']].style.format({'permalink': make_clickable}))\n",
    "    elif ANALYSIS == 'topic model':\n",
    "        idx = df['topic'] == int(contains[:contains.find(':')])\n",
    "        print(f'Found {idx.sum()} tweets')\n",
    "        display(df.loc[idx,['username','to','text','topic','permalink']].style.format({'permalink': make_clickable}))\n",
    "\n",
    "w = widgets.Select(options=items)\n",
    "interactive(view, contains=w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('ppp': conda)",
   "language": "python",
   "name": "python361064bitpppconda08cb32434e5c43ab89b88a4949d5ebb2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
