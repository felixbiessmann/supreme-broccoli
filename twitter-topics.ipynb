{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import GetOldTweets3 as got \n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "from ipywidgets import widgets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "global KEYWORDS\n",
    "global DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_url(url):\n",
    "    if url:\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "            return \" \".join([p.get_text().replace(u'\\xa0', u' ') for p in soup.find_all('p')])\n",
    "        except:\n",
    "            return \"\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topics[topic_idx] = \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return topics\n",
    "\n",
    "def get_topics(texts, \n",
    "               n_components=20, \n",
    "               n_top_words = 20, \n",
    "               n_features=int(1e12),\n",
    "               keywords=[],\n",
    "               topic_model='NMF'\n",
    "              ):\n",
    "    vect = CountVectorizer(max_df=0.8, \n",
    "                           min_df=1,\n",
    "                           max_features=n_features, \n",
    "                           stop_words=get_stop_words('de') + keywords)\n",
    "    if topic_model == 'LDA':\n",
    "        model = LatentDirichletAllocation(n_components=n_components, max_iter=200,\n",
    "                                learning_method='online',\n",
    "                                random_state=0)\n",
    "    elif topic_model == 'NMF':\n",
    "        model = NMF(n_components=n_components, random_state=0)\n",
    "    tf = vect.fit_transform(texts)\n",
    "    model.fit(tf)\n",
    "    topic_assignments = model.transform(tf)\n",
    "    tf_feature_names = vect.get_feature_names()\n",
    "    topics_dict = get_top_words(model, tf_feature_names, n_top_words)\n",
    "    \n",
    "    # sort topics by occurrence\n",
    "    topic_loadings = topic_assignments.sum(axis=0).argsort()[:-1:]\n",
    "    topic_assignments = topic_assignments[:,topic_loadings]\n",
    "    topics = [topics_dict[topic_idx] for topic_idx in topic_loadings]\n",
    "    return topic_assignments.argmax(axis=1), topics\n",
    "\n",
    "def get_top_word_counts(texts, n_top_words, keywords):\n",
    "    vect = CountVectorizer(max_df=1., \n",
    "                           min_df=1,\n",
    "                           max_features=n_top_words, \n",
    "                           stop_words=get_stop_words('de') + keywords)\n",
    "    wordcounts = vect.fit_transform(texts)\n",
    "    return wordcounts, vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93b89e7933a4730a10e9637db1eb5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='covid klopapier', description='keywords'), DatePicker(value=Timestamp('2020-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact_manual(maxTweets=(0, 100))\n",
    "def get_tweets(keywords='covid klopapier', \n",
    "               start=widgets.DatePicker(value=pd.to_datetime('2020-04-10')), \n",
    "               stop=widgets.DatePicker(value=pd.to_datetime('2020-04-16')), \n",
    "               maxTweets=10):\n",
    "    print(f'Fetching tweets for keywords: {keywords}')\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(keywords)\\\n",
    "                                             .setSince(start.strftime('%Y-%m-%d'))\\\n",
    "                                             .setUntil(stop.strftime('%Y-%m-%d'))\\\n",
    "                                             .setMaxTweets(maxTweets)\\\n",
    "                                             .setLang('de')\\\n",
    "                                             .setNear('Berlin, Germany')\\\n",
    "                                             .setWithin('1000km') \n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "    tweet_dicts = []\n",
    "    for tweet in tweets:\n",
    "        tweet_dict = tweet.__dict__ \n",
    "        tweet_dict['url_text'] = get_text_from_url(tweet.urls)\n",
    "        tweet_dicts.append(tweet_dict)\n",
    "        \n",
    "    print(f'Found {len(tweets)} tweets for keywords: {keywords}')\n",
    "    global DF\n",
    "    DF = pd.DataFrame(tweet_dicts).set_index('date')\n",
    "    global KEYWORDS\n",
    "    KEYWORDS = re.split(\"[\" + string.punctuation + \" \\t\\n]+\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223ed7fd7ed94048870a806ca4c3970f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='num_topics', max=20, min=2), Dropdown(description='text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact_manual(num_topics=(2, 20),\n",
    "                         text_source=['tweets','urls in tweets'],\n",
    "                         analysis=['topic model', 'wordcounts'],\n",
    "                         n_top_words = (5, 20),\n",
    "                         topic_model=['LDA','NMF']\n",
    "                        )\n",
    "def plot_tweets(\n",
    "         num_topics=10,\n",
    "         text_source='tweets',\n",
    "         analysis='topic model',\n",
    "         n_top_words=5,\n",
    "         topic_model='NMF'\n",
    "        ):\n",
    "    df = DF.copy(deep=True)\n",
    "    if len(df) == 0:\n",
    "        print(\"Get some tweets first.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Analysing {len(df)} {text_source} with {analysis}, exluding keywords: {KEYWORDS}\")\n",
    "        \n",
    "        if text_source == 'tweets':\n",
    "            text_col = 'text'\n",
    "        elif text_source == 'urls in tweets':            \n",
    "            text_col = 'url_text'\n",
    "            \n",
    "        if analysis == 'topic model':\n",
    "            df['topics'], text_topics = get_topics(df[text_col], \n",
    "                                                    n_components=num_topics, \n",
    "                                                    n_top_words=n_top_words,\n",
    "                                                    keywords=KEYWORDS,\n",
    "                                                    topic_model=topic_model\n",
    "                                                  )\n",
    "            pd.get_dummies(df['topics']).resample('D').sum().plot(marker='o')\n",
    "            plt.legend([f'Topic {idx}: {text[:10]} ...' for idx, text in enumerate(text_topics)]);\n",
    "            print(\"\\n\".join([f'Topic {idx}: {text}' for idx, text in enumerate(text_topics)]))\n",
    "        elif analysis == 'wordcounts':\n",
    "            word_counts, words = get_top_word_counts(df[text_col],  \n",
    "                                                     n_top_words=n_top_words,\n",
    "                                                    keywords=KEYWORDS)\n",
    "            \n",
    "            pd.DataFrame(word_counts.toarray(), \n",
    "                         index=df.index, \n",
    "                         columns=words).resample('D').sum().plot(marker='o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('ppp': conda)",
   "language": "python",
   "name": "python361064bitpppconda08cb32434e5c43ab89b88a4949d5ebb2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
