{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, os, glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import GetOldTweets3 as got \n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "from ipywidgets import widgets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "global KEYWORDS\n",
    "global DF\n",
    "global ANALYSIS\n",
    "global TEXT_SOURCE\n",
    "global WORDS\n",
    "global TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_url(url):\n",
    "    if url:\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "            return \" \".join([p.get_text().replace(u'\\xa0', u' ') for p in soup.find_all('p')])\n",
    "        except:\n",
    "            return \"\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topics[topic_idx] = \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return topics\n",
    "\n",
    "def get_topics(texts, \n",
    "               n_components=20, \n",
    "               n_top_words = 20, \n",
    "               n_features=int(1e12),\n",
    "               keywords=[],\n",
    "               topic_model='NMF'\n",
    "              ):\n",
    "    vect = CountVectorizer(max_df=0.8, \n",
    "                           min_df=1,\n",
    "                           max_features=n_features, \n",
    "                           stop_words=get_stop_words('de') + keywords)\n",
    "    if topic_model == 'LDA':\n",
    "        model = LatentDirichletAllocation(n_components=n_components, max_iter=200,\n",
    "                                learning_method='online',\n",
    "                                random_state=0)\n",
    "    elif topic_model == 'NMF':\n",
    "        model = NMF(n_components=n_components, random_state=0)\n",
    "    tf = vect.fit_transform(texts)\n",
    "    model.fit(tf)\n",
    "    topic_assignments = model.transform(tf)\n",
    "    tf_feature_names = vect.get_feature_names()\n",
    "    topics_dict = get_top_words(model, tf_feature_names, n_top_words)\n",
    "    \n",
    "    # sort topics by occurrence\n",
    "    topic_loadings = topic_assignments.sum(axis=0).argsort()[:-1:]\n",
    "    topic_assignments = topic_assignments[:,topic_loadings]\n",
    "    topics = [topics_dict[topic_idx] for topic_idx in topic_loadings]\n",
    "    return topic_assignments.argmax(axis=1), topics\n",
    "\n",
    "def get_top_word_counts(texts, n_top_words, keywords):\n",
    "    vect = CountVectorizer(max_df=1., \n",
    "                           min_df=1,\n",
    "                           max_features=n_top_words, \n",
    "                           stop_words=get_stop_words('de') + keywords)\n",
    "    wordcounts = vect.fit_transform(texts)\n",
    "    return wordcounts, vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Analyse mit Topic Models (oder ohne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Runterladen\n",
    "\n",
    "Lade Tweets aus deutschsprachigem Raum für den angegebenen Zeitraum, die die angegebenen keywords enthalten.\n",
    "\n",
    "Wenn maxTweets auf 0 gesetzt ist, wird versucht alle zu holen. Das kann dauern, und manchmal ist man danach gedrosselt. \n",
    "\n",
    "Klicke ``Run Interact`` button zum ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7e3e75a7644452a494966b12b4fbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='covid klopapier', description='keywords'), DatePicker(value=Timestamp('2020-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact_manual(maxTweets=(0, 100))\n",
    "def get_tweets(keywords='covid klopapier', \n",
    "               start=widgets.DatePicker(value=pd.to_datetime('2020-04-10')), \n",
    "               stop=widgets.DatePicker(value=pd.to_datetime('2020-04-16')), \n",
    "               maxTweets=10):\n",
    "    print(f'Fetching tweets for keywords: {keywords}')\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(keywords)\\\n",
    "                                             .setSince(start.strftime('%Y-%m-%d'))\\\n",
    "                                             .setUntil(stop.strftime('%Y-%m-%d'))\\\n",
    "                                             .setMaxTweets(maxTweets)\\\n",
    "                                             .setLang('de')\\\n",
    "                                             .setNear('Berlin, Germany')\\\n",
    "                                             .setWithin('1000km') \n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "    tweet_dicts = []\n",
    "    for tweet in tweets:\n",
    "        tweet_dict = tweet.__dict__ \n",
    "        tweet_dict['url_text'] = get_text_from_url(tweet.urls)\n",
    "        tweet_dicts.append(tweet_dict)\n",
    "        \n",
    "    print(f'Found {len(tweets)} tweets for keywords: {keywords}')\n",
    "    global DF\n",
    "    DF = pd.DataFrame(tweet_dicts).set_index('date')\n",
    "    global KEYWORDS\n",
    "    KEYWORDS = re.split(\"[\" + string.punctuation + \" \\t\\n]+\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Analysieren\n",
    "\n",
    "- ``text_source``: Wähle, ob Text aus tweets oder den in tweets verlinkten Artikeln analysiert werden soll. \n",
    "- ``analysis``: Wähle ob Häufigkeit einzelner Worte oder Topics analysiert werden.\n",
    "- ``n_top_words``: wie viele Worte berücksichtigt werden oder bei Topic Analysis, wieviele Wòrter angezeigt werden\n",
    "- ``topic_model``: Art des Topic Models (Latent Dirichlet Allocation oder Nonnegative Matrix Factorization) - wird ignoriert, wnen ``analysis`` auf ``wordcount`` gesetzt ist\n",
    "- ``num_topics``: Wie viele Topics extrahiert werden soll.\n",
    "\n",
    "Klicke ``Run Interact`` button zum ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6827ef36ce0f4a14932009bec4795ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='text_source', options=('tweets', 'urls in tweets'), value='tweets'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact_manual(num_topics=(2, 20),\n",
    "                         text_source=['tweets','urls in tweets'],\n",
    "                         analysis=['topic model', 'wordcounts'],\n",
    "                         n_top_words = (5, 20),\n",
    "                         topic_model=['LDA','NMF']\n",
    "                        )\n",
    "def plot_tweets(\n",
    "         text_source='tweets',\n",
    "         analysis='topic model',\n",
    "         n_top_words=5,\n",
    "         topic_model='NMF',\n",
    "         num_topics=10    \n",
    "        ):\n",
    "    global DF\n",
    "    global TOPICS\n",
    "    global ANALYSIS \n",
    "    ANALYSIS = analysis\n",
    "    global TEXT_SOURCE\n",
    "    TEXT_SOURCE = text_source\n",
    "    if len(DF) == 0:\n",
    "        print(\"Get some tweets first.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Analysing {len(DF)} {text_source} with {analysis}, exluding keywords: {KEYWORDS}\")\n",
    "        \n",
    "        if text_source == 'tweets':\n",
    "            text_col = 'text'\n",
    "        elif text_source == 'urls in tweets':            \n",
    "            text_col = 'url_text'\n",
    "            \n",
    "        if analysis == 'topic model':\n",
    "            DF['topic'], TOPICS = get_topics(DF[text_col], \n",
    "                                                    n_components=num_topics, \n",
    "                                                    n_top_words=n_top_words,\n",
    "                                                    keywords=KEYWORDS,\n",
    "                                                    topic_model=topic_model\n",
    "                                                  )\n",
    "            pd.get_dummies(DF['topic']).resample('D').sum().plot(marker='o')\n",
    "            plt.legend([f'Topic {idx}: {text[:10]} ...' for idx, text in enumerate(TOPICS)]);\n",
    "            print(\"\\n\".join([f'Topic {idx}: {text}' for idx, text in enumerate(TOPICS)]))\n",
    "        elif analysis == 'wordcounts':\n",
    "            word_counts, words = get_top_word_counts(DF[text_col],  \n",
    "                                                     n_top_words=n_top_words,\n",
    "                                                    keywords=KEYWORDS)\n",
    "            global TOP_WORDS\n",
    "            TOP_WORDS = words\n",
    "            pd.DataFrame(word_counts.toarray(), \n",
    "                         index=DF.index, \n",
    "                         columns=words).resample('D').sum().plot(marker='o')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Anschauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5977bbc960664f64bdb8f010d8e50998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Select(description='contains', options=('All',), value='All'), Output()), _dom_classes=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.html.widgets import interactive\n",
    "\n",
    "try:\n",
    "    df = DF.copy(deep=True)\n",
    "except NameError:\n",
    "    DF = pd.DataFrame([],columns=['username','to','text','topic','permalink'])\n",
    "    df = DF.copy(deep=True)\n",
    "    TEXT_SOURCE = 'tweets'\n",
    "    ANALYSIS = 'wordcounts'\n",
    "    TOP_WORDS = []\n",
    "\n",
    "if TEXT_SOURCE == 'tweets':\n",
    "    text_col = 'text'\n",
    "elif TEXT_SOURCE == 'urls in tweets':\n",
    "    text_col = 'url_text'\n",
    "\n",
    "if ANALYSIS == 'wordcounts':\n",
    "    items = ['All'] + sorted(TOP_WORDS)\n",
    "elif ANALYSIS == 'topic model':\n",
    "    items = ['All'] + [f'{idx}: {text[:20]} ...' for idx, text in enumerate(TOPICS)]\n",
    "    \n",
    "def make_clickable(val):\n",
    "    # target _blank to open new window\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val, val)\n",
    "    \n",
    "def view(contains=''):\n",
    "    if contains=='All': \n",
    "        print(f'Showing all {len(df)} tweets')\n",
    "        display(df.loc[:,['username','to','text','permalink']].style.format({'permalink': make_clickable}))\n",
    "    elif ANALYSIS == 'wordcounts':\n",
    "        idx = df[text_col].str.lower().str.contains(contains.lower())\n",
    "        print(f'Found {idx.sum()} tweets')\n",
    "        display(df.loc[idx,['username','to','text','permalink']].style.format({'permalink': make_clickable}))\n",
    "    elif ANALYSIS == 'topic model':\n",
    "        idx = df['topic'] == int(contains[:contains.find(':')])\n",
    "        print(f'Found {idx.sum()} tweets')\n",
    "        display(df.loc[idx,['username','to','text','topic','permalink']].style.format({'permalink': make_clickable}))\n",
    "\n",
    "w = widgets.Select(options=items)\n",
    "interactive(view, contains=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6401762b9a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtweet_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twitterdata'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'*.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ppp/lib/python3.6/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppp/lib/python3.6/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppp/lib/python3.6/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppp/lib/python3.6/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppp/lib/python3.6/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1093\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             )\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "tweet_files = glob.glob(os.path.join('twitterdata','*','*.json'))\n",
    "df_ = pd.read_json(tweet_files[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>replies</th>\n",
       "      <th>id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>author_id</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>geo</th>\n",
       "      <th>urls</th>\n",
       "      <th>url_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-15 23:37:15+00:00</th>\n",
       "      <td>vv_viktoria</td>\n",
       "      <td>None</td>\n",
       "      <td>Die neue Therapie mit #Blutplasmaspende an der...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1250568904008708096</td>\n",
       "      <td>https://twitter.com/vv_viktoria/status/1250568...</td>\n",
       "      <td>445775804</td>\n",
       "      <td>Wed Apr 15 23:37:15 +0000 2020</td>\n",
       "      <td>#Blutplasmaspende #TogetherWeRise #covid</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-15 22:58:38+00:00</th>\n",
       "      <td>DamarisSieglind</td>\n",
       "      <td>sofatiere</td>\n",
       "      <td>Sie unterliegen leider den finanziellen Zwänge...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1250559186217435142</td>\n",
       "      <td>https://twitter.com/DamarisSieglind/status/125...</td>\n",
       "      <td>937754666401755136</td>\n",
       "      <td>Wed Apr 15 22:58:38 +0000 2020</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-15 21:32:06+00:00</th>\n",
       "      <td>PeterHeidtFDP</td>\n",
       "      <td>c_lindner</td>\n",
       "      <td>Aktuell sinken die Zahlen der Neuinfektionen, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1250537408409600000</td>\n",
       "      <td>https://twitter.com/PeterHeidtFDP/status/12505...</td>\n",
       "      <td>1858621434</td>\n",
       "      <td>Wed Apr 15 21:32:06 +0000 2020</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-15 21:01:39+00:00</th>\n",
       "      <td>Knippkopf</td>\n",
       "      <td>lange_tobias_hh</td>\n",
       "      <td>Jetzt komm mir noch mit \"Es gab sogar Klopapie...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1250529746657689603</td>\n",
       "      <td>https://twitter.com/Knippkopf/status/125052974...</td>\n",
       "      <td>2305238089</td>\n",
       "      <td>Wed Apr 15 21:01:39 +0000 2020</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-15 20:30:04+00:00</th>\n",
       "      <td>JoergRMayer</td>\n",
       "      <td>SimonKohlbauer</td>\n",
       "      <td>Was das mit rechts und links zu tun hat, ersch...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1250521798141194240</td>\n",
       "      <td>https://twitter.com/JoergRMayer/status/1250521...</td>\n",
       "      <td>3008773191</td>\n",
       "      <td>Wed Apr 15 20:30:04 +0000 2020</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10 05:02:10+00:00</th>\n",
       "      <td>StefanKuster</td>\n",
       "      <td>BAG_OFSP_UFSP</td>\n",
       "      <td>Peinlicher geht nimmer @BAG_OFSP_UFSP Löffel e...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1248476341747331078</td>\n",
       "      <td>https://twitter.com/StefanKuster/status/124847...</td>\n",
       "      <td>292798962</td>\n",
       "      <td>Fri Apr 10 05:02:10 +0000 2020</td>\n",
       "      <td>#peinlich #Covid_19</td>\n",
       "      <td>@BAG_OFSP_UFSP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10 02:58:58+00:00</th>\n",
       "      <td>nice_pebbles</td>\n",
       "      <td>None</td>\n",
       "      <td>Guten Morgen und frohe #Ostern trotz (oder ger...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1248445340493262849</td>\n",
       "      <td>https://twitter.com/nice_pebbles/status/124844...</td>\n",
       "      <td>4219168216</td>\n",
       "      <td>Fri Apr 10 02:58:58 +0000 2020</td>\n",
       "      <td>#Ostern #COVID #stabil #sge #eintracht</td>\n",
       "      <td>@Eintracht</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10 02:37:02+00:00</th>\n",
       "      <td>covid_watch</td>\n",
       "      <td>None</td>\n",
       "      <td>Coronavirus fordert Pflegedienste</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1248439819417423872</td>\n",
       "      <td>https://twitter.com/covid_watch/status/1248439...</td>\n",
       "      <td>1240313872927653892</td>\n",
       "      <td>Fri Apr 10 02:37:02 +0000 2020</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>http://dlvr.it/RTVzPB</td>\n",
       "      <td>\\n                NÖN.at verwendet Cookies, um...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10 01:46:03+00:00</th>\n",
       "      <td>covid_watch</td>\n",
       "      <td>None</td>\n",
       "      <td>Restaurant Speisekammer West in 70193 Stuttga...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1248426987057598466</td>\n",
       "      <td>https://twitter.com/covid_watch/status/1248426...</td>\n",
       "      <td>1240313872927653892</td>\n",
       "      <td>Fri Apr 10 01:46:03 +0000 2020</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>http://dlvr.it/RTVvKT</td>\n",
       "      <td>Beste lokale Produzenten liefern für die regio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10 00:29:31+00:00</th>\n",
       "      <td>L3tsMax</td>\n",
       "      <td>None</td>\n",
       "      <td>Sieh Dir den Fundraiser von @mikeshinoda an, u...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1248407730315902979</td>\n",
       "      <td>https://twitter.com/L3tsMax/status/12484077303...</td>\n",
       "      <td>2369035838</td>\n",
       "      <td>Fri Apr 10 00:29:31 +0000 2020</td>\n",
       "      <td></td>\n",
       "      <td>@mikeshinoda @WeAreTiltify</td>\n",
       "      <td></td>\n",
       "      <td>https://donate.tiltify.com/@mikeshinoda/ms-c19</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>685 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  username               to  \\\n",
       "date                                                          \n",
       "2020-04-15 23:37:15+00:00      vv_viktoria             None   \n",
       "2020-04-15 22:58:38+00:00  DamarisSieglind        sofatiere   \n",
       "2020-04-15 21:32:06+00:00    PeterHeidtFDP        c_lindner   \n",
       "2020-04-15 21:01:39+00:00        Knippkopf  lange_tobias_hh   \n",
       "2020-04-15 20:30:04+00:00      JoergRMayer   SimonKohlbauer   \n",
       "...                                    ...              ...   \n",
       "2020-04-10 05:02:10+00:00     StefanKuster    BAG_OFSP_UFSP   \n",
       "2020-04-10 02:58:58+00:00     nice_pebbles             None   \n",
       "2020-04-10 02:37:02+00:00      covid_watch             None   \n",
       "2020-04-10 01:46:03+00:00      covid_watch             None   \n",
       "2020-04-10 00:29:31+00:00          L3tsMax             None   \n",
       "\n",
       "                                                                        text  \\\n",
       "date                                                                           \n",
       "2020-04-15 23:37:15+00:00  Die neue Therapie mit #Blutplasmaspende an der...   \n",
       "2020-04-15 22:58:38+00:00  Sie unterliegen leider den finanziellen Zwänge...   \n",
       "2020-04-15 21:32:06+00:00  Aktuell sinken die Zahlen der Neuinfektionen, ...   \n",
       "2020-04-15 21:01:39+00:00  Jetzt komm mir noch mit \"Es gab sogar Klopapie...   \n",
       "2020-04-15 20:30:04+00:00  Was das mit rechts und links zu tun hat, ersch...   \n",
       "...                                                                      ...   \n",
       "2020-04-10 05:02:10+00:00  Peinlicher geht nimmer @BAG_OFSP_UFSP Löffel e...   \n",
       "2020-04-10 02:58:58+00:00  Guten Morgen und frohe #Ostern trotz (oder ger...   \n",
       "2020-04-10 02:37:02+00:00                  Coronavirus fordert Pflegedienste   \n",
       "2020-04-10 01:46:03+00:00   Restaurant Speisekammer West in 70193 Stuttga...   \n",
       "2020-04-10 00:29:31+00:00  Sieh Dir den Fundraiser von @mikeshinoda an, u...   \n",
       "\n",
       "                           retweets  favorites  replies                   id  \\\n",
       "date                                                                           \n",
       "2020-04-15 23:37:15+00:00         0          0        0  1250568904008708096   \n",
       "2020-04-15 22:58:38+00:00         0          2        0  1250559186217435142   \n",
       "2020-04-15 21:32:06+00:00         1          1        0  1250537408409600000   \n",
       "2020-04-15 21:01:39+00:00         0          2        0  1250529746657689603   \n",
       "2020-04-15 20:30:04+00:00         0          2        0  1250521798141194240   \n",
       "...                             ...        ...      ...                  ...   \n",
       "2020-04-10 05:02:10+00:00         0          3        0  1248476341747331078   \n",
       "2020-04-10 02:58:58+00:00         0         18        0  1248445340493262849   \n",
       "2020-04-10 02:37:02+00:00         1          1        0  1248439819417423872   \n",
       "2020-04-10 01:46:03+00:00         0          0        0  1248426987057598466   \n",
       "2020-04-10 00:29:31+00:00         0          0        0  1248407730315902979   \n",
       "\n",
       "                                                                   permalink  \\\n",
       "date                                                                           \n",
       "2020-04-15 23:37:15+00:00  https://twitter.com/vv_viktoria/status/1250568...   \n",
       "2020-04-15 22:58:38+00:00  https://twitter.com/DamarisSieglind/status/125...   \n",
       "2020-04-15 21:32:06+00:00  https://twitter.com/PeterHeidtFDP/status/12505...   \n",
       "2020-04-15 21:01:39+00:00  https://twitter.com/Knippkopf/status/125052974...   \n",
       "2020-04-15 20:30:04+00:00  https://twitter.com/JoergRMayer/status/1250521...   \n",
       "...                                                                      ...   \n",
       "2020-04-10 05:02:10+00:00  https://twitter.com/StefanKuster/status/124847...   \n",
       "2020-04-10 02:58:58+00:00  https://twitter.com/nice_pebbles/status/124844...   \n",
       "2020-04-10 02:37:02+00:00  https://twitter.com/covid_watch/status/1248439...   \n",
       "2020-04-10 01:46:03+00:00  https://twitter.com/covid_watch/status/1248426...   \n",
       "2020-04-10 00:29:31+00:00  https://twitter.com/L3tsMax/status/12484077303...   \n",
       "\n",
       "                                     author_id  \\\n",
       "date                                             \n",
       "2020-04-15 23:37:15+00:00            445775804   \n",
       "2020-04-15 22:58:38+00:00   937754666401755136   \n",
       "2020-04-15 21:32:06+00:00           1858621434   \n",
       "2020-04-15 21:01:39+00:00           2305238089   \n",
       "2020-04-15 20:30:04+00:00           3008773191   \n",
       "...                                        ...   \n",
       "2020-04-10 05:02:10+00:00            292798962   \n",
       "2020-04-10 02:58:58+00:00           4219168216   \n",
       "2020-04-10 02:37:02+00:00  1240313872927653892   \n",
       "2020-04-10 01:46:03+00:00  1240313872927653892   \n",
       "2020-04-10 00:29:31+00:00           2369035838   \n",
       "\n",
       "                                           formatted_date  \\\n",
       "date                                                        \n",
       "2020-04-15 23:37:15+00:00  Wed Apr 15 23:37:15 +0000 2020   \n",
       "2020-04-15 22:58:38+00:00  Wed Apr 15 22:58:38 +0000 2020   \n",
       "2020-04-15 21:32:06+00:00  Wed Apr 15 21:32:06 +0000 2020   \n",
       "2020-04-15 21:01:39+00:00  Wed Apr 15 21:01:39 +0000 2020   \n",
       "2020-04-15 20:30:04+00:00  Wed Apr 15 20:30:04 +0000 2020   \n",
       "...                                                   ...   \n",
       "2020-04-10 05:02:10+00:00  Fri Apr 10 05:02:10 +0000 2020   \n",
       "2020-04-10 02:58:58+00:00  Fri Apr 10 02:58:58 +0000 2020   \n",
       "2020-04-10 02:37:02+00:00  Fri Apr 10 02:37:02 +0000 2020   \n",
       "2020-04-10 01:46:03+00:00  Fri Apr 10 01:46:03 +0000 2020   \n",
       "2020-04-10 00:29:31+00:00  Fri Apr 10 00:29:31 +0000 2020   \n",
       "\n",
       "                                                           hashtags  \\\n",
       "date                                                                  \n",
       "2020-04-15 23:37:15+00:00  #Blutplasmaspende #TogetherWeRise #covid   \n",
       "2020-04-15 22:58:38+00:00                                             \n",
       "2020-04-15 21:32:06+00:00                                             \n",
       "2020-04-15 21:01:39+00:00                                             \n",
       "2020-04-15 20:30:04+00:00                                             \n",
       "...                                                             ...   \n",
       "2020-04-10 05:02:10+00:00                       #peinlich #Covid_19   \n",
       "2020-04-10 02:58:58+00:00    #Ostern #COVID #stabil #sge #eintracht   \n",
       "2020-04-10 02:37:02+00:00                                             \n",
       "2020-04-10 01:46:03+00:00                                             \n",
       "2020-04-10 00:29:31+00:00                                             \n",
       "\n",
       "                                             mentions geo  \\\n",
       "date                                                        \n",
       "2020-04-15 23:37:15+00:00                                   \n",
       "2020-04-15 22:58:38+00:00                                   \n",
       "2020-04-15 21:32:06+00:00                                   \n",
       "2020-04-15 21:01:39+00:00                                   \n",
       "2020-04-15 20:30:04+00:00                                   \n",
       "...                                               ...  ..   \n",
       "2020-04-10 05:02:10+00:00              @BAG_OFSP_UFSP       \n",
       "2020-04-10 02:58:58+00:00                  @Eintracht       \n",
       "2020-04-10 02:37:02+00:00                                   \n",
       "2020-04-10 01:46:03+00:00                                   \n",
       "2020-04-10 00:29:31+00:00  @mikeshinoda @WeAreTiltify       \n",
       "\n",
       "                                                                     urls  \\\n",
       "date                                                                        \n",
       "2020-04-15 23:37:15+00:00                                                   \n",
       "2020-04-15 22:58:38+00:00                                                   \n",
       "2020-04-15 21:32:06+00:00                                                   \n",
       "2020-04-15 21:01:39+00:00                                                   \n",
       "2020-04-15 20:30:04+00:00                                                   \n",
       "...                                                                   ...   \n",
       "2020-04-10 05:02:10+00:00                                                   \n",
       "2020-04-10 02:58:58+00:00                                                   \n",
       "2020-04-10 02:37:02+00:00                           http://dlvr.it/RTVzPB   \n",
       "2020-04-10 01:46:03+00:00                           http://dlvr.it/RTVvKT   \n",
       "2020-04-10 00:29:31+00:00  https://donate.tiltify.com/@mikeshinoda/ms-c19   \n",
       "\n",
       "                                                                    url_text  \n",
       "date                                                                          \n",
       "2020-04-15 23:37:15+00:00                                                     \n",
       "2020-04-15 22:58:38+00:00                                                     \n",
       "2020-04-15 21:32:06+00:00                                                     \n",
       "2020-04-15 21:01:39+00:00                                                     \n",
       "2020-04-15 20:30:04+00:00                                                     \n",
       "...                                                                      ...  \n",
       "2020-04-10 05:02:10+00:00                                                     \n",
       "2020-04-10 02:58:58+00:00                                                     \n",
       "2020-04-10 02:37:02+00:00  \\n                NÖN.at verwendet Cookies, um...  \n",
       "2020-04-10 01:46:03+00:00  Beste lokale Produzenten liefern für die regio...  \n",
       "2020-04-10 00:29:31+00:00                                                     \n",
       "\n",
       "[685 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading manifesto/manifesto-Germany.csv\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    3.0s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    0.8s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    5.1s remaining:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    5.2s finished\n"
     ]
    }
   ],
   "source": [
    "from classifier import *\n",
    "train_all()\n",
    "df = get_bundestag_data()\n",
    "df = score_texts(df)\n",
    "df.to_csv(\"bundestagsprotokolle/bundestagsprotokolle_categorized.csv.gz\",index=False,compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('ppp': conda)",
   "language": "python",
   "name": "python361064bitpppconda08cb32434e5c43ab89b88a4949d5ebb2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
